{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "f3mGGGa1h3-z"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOt9AeckWb4hLgdlvNCGpRx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku5awU-ld9rW"
      },
      "source": [
        "#WordNet Extractor\n",
        "Extract Definitions, examples and words from WordNet, for a given set of seed words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqoPMAh64SWu"
      },
      "source": [
        "####Libraries to import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ_sRt9BYJAR"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "#nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "import os\n",
        "from urllib.request import Request, urlopen, URLError\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "from wordnik import *\n",
        "import swifter\n",
        "from tqdm import tqdm \n",
        "import matplotlib.pyplot as plt\n",
        "import timeit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9idEcUk4Y1H"
      },
      "source": [
        "#### Global Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHB0wzWT8lSW"
      },
      "source": [
        "\"\"\"\n",
        "Global Params\n",
        "\"\"\"\n",
        "#file structures \n",
        "rootdir = 'D:\\HWU\\F21MP'\n",
        "datadir = 'data'\n",
        "tqdm.pandas()\n",
        "#access the key files\n",
        "wordnikKey = open(rootdir + os.sep + \"api_key\\wordnik.key\", \"r\").read()\n",
        "wordrec = {\n",
        "              \"theme\"       : ''\n",
        "           ,  \"parent_word\" : ''\n",
        "           ,  \"confidence\"  : ''\n",
        "           ,  \"synset_name\" : ''\n",
        "           ,  \"lex\"         : ''\n",
        "           ,  \"pos\"         : ''\n",
        "           ,  \"source\"      : 'wordnet' \n",
        "           ,  \"ttype\"       : ''\n",
        "           ,  \"text\"        : ''\n",
        "    }\n",
        "wordlist = []\n",
        "master_columns=['theme', 'parent_word', 'confidence', 'synset_name', 'lex', 'pos', 'text']\n",
        "tdf = pd.DataFrame(columns=master_columns)\n",
        "wa = pd.DataFrame(columns=master_columns) #workarea"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYSKfW4w4lAi"
      },
      "source": [
        "####Subroutines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8E5ZZeb4v50"
      },
      "source": [
        "##### Extraction from other dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRMg5b31zSL1"
      },
      "source": [
        "def write_wordlist_to_file(theme, filename, reclist=wordlist):\n",
        "  \"\"\"\n",
        "  append wordlist to dataframe and write it to file\n",
        "\n",
        "  Input:\n",
        "    df(pandas dataframe) \n",
        "    file(filename)\n",
        "    reclist(list of word records)\n",
        "  Output:\n",
        "    new_df(pandas dataframe) after appending the records to dataframe df\n",
        "  \"\"\"\n",
        "  if len(wa) > 0:\n",
        "    my_theme = wa.theme[0]\n",
        "  else:\n",
        "     my_theme = theme\n",
        "  if my_theme!=theme:\n",
        "    wa = wa[0:0]\n",
        "  if len(reclist) > 0:\n",
        "    wa = wa.append(reclist)\n",
        "    wa.to_csv(filename)\n",
        "    print('**Checkpoint reached. List saved to ,', filename)\n",
        "    print('List length = ', len(reclist))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxJJGz0pCyyL"
      },
      "source": [
        "def get_examples_from_yourdictionary( word , elimit=5):\n",
        "  \"\"\"\n",
        "  This method scraps examples from yourdictionary.com for the given word\n",
        "  INPUT:\n",
        "    word =word to be extracted\n",
        "    elimit = max number of sentences\n",
        "  OUTPUT:\n",
        "    sentence_ = list of sentences\n",
        "  \"\"\"\n",
        "  base_url = 'https://sentence.yourdictionary.com/'\n",
        "  word = word.replace('_', '-')\n",
        "  url = base_url + word\n",
        "  req = Request(url)                   # grab web page\n",
        "  sentences_ = []\n",
        "  try:\n",
        "        grab_page = urlopen(req).read()\n",
        "  except URLError as e:\n",
        "      if hasattr(e, 'reason'):\n",
        "          print(word, e.reason)\n",
        "\n",
        "      elif hasattr(e, 'code'):\n",
        "        print('The server couldn\\'t fulfill the request.')\n",
        "        print('Error code: ', e.code)\n",
        "  except exception as e2:\n",
        "    print('yourdictionary Error code: ', e2.code, ': word :', word)\n",
        "  else:\n",
        "        web_page = BeautifulSoup(grab_page, 'html.parser') # read web page lines\n",
        "        #print(web_page)\n",
        "        divs = web_page.find_all(\"div\", class_= \"sentence component\")\n",
        "        if divs:\n",
        "          for index, div in enumerate(divs):\n",
        "            if index >= elimit: \n",
        "              break\n",
        "            sentences_.append(div.get_text())\n",
        "        else: \n",
        "          index = 0\n",
        "          json_lines = web_page.find_all('script')\n",
        "          for json_line in json_lines:\n",
        "            json_line = json_line.decode(eventual_encoding=\"utf-8\").replace('\\\\\"','\"')\n",
        "            json_line = json_line.replace('\\\\\\\\\\\"', '')\n",
        "            if 'sentences' in json_line:\n",
        "              try:\n",
        "                json_line = json_line.split('JSON.parse',1)[1]\n",
        "                json_line = json_line[2:]\n",
        "                json_line = json_line[:-12] \n",
        "                jsonData = json.loads(json_line)\n",
        "                jsonData = jsonData['data']['data']['sentences']\n",
        "              except Exception as e:\n",
        "                print('yourdictionary.com: Error extracting examples for :', word)\n",
        "              else:\n",
        "                for sentence in jsonData:\n",
        "                  sentences_.append(sentence['sentence'])   \n",
        "                  index = index + 1\n",
        "                  if index >= elimit:\n",
        "                    return sentences_\n",
        "  return sentences_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqlEOXPe7Mz2"
      },
      "source": [
        "def get_examples_from_wordnik( word, elimit=5):\n",
        "  \"\"\"\n",
        "  This gets examples for wordnik.\n",
        "  INPUT: word = word for which example needs to be extracted\n",
        "         elimit = max limit of examples to be extracted per word\n",
        "  OUTPUT: sentences_ = list of sentences\n",
        "  \"\"\"\n",
        "  sentences_ = []\n",
        "  apiUrl = 'http://api.wordnik.com/v4'\n",
        "  client = swagger.ApiClient(wordnikKey, apiUrl)\n",
        "  wordApi = WordApi.WordApi(client)\n",
        "  try:\n",
        "    examplesSet = wordApi.getExamples(word, limit= elimit, includeDuplicates=False)\n",
        "  except URLError as e:\n",
        "      if hasattr(e, 'reason'):\n",
        "          print(word, e.reason)\n",
        "\n",
        "      elif hasattr(e, 'code'):\n",
        "        print('The server couldn\\'t fulfill the request.')\n",
        "        print('Error code: ', e.code)\n",
        "  except Exception as e:\n",
        "      if hasattr(e, 'reason'):\n",
        "          print(word, e.reason)\n",
        "      elif hasattr(e, 'code'):\n",
        "        print('The server couldn\\'t fulfill the request.')\n",
        "        print('Error code: ', e.code)\n",
        "      else:\n",
        "        print('Error processing, word:', word)  \n",
        "  else:\n",
        "    sentences_ = [example.text for example in examplesSet.examples]\n",
        "  return sentences_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ljwdEmxkBiA"
      },
      "source": [
        "def get_examples_by_wordrec(method, df, filename, rec, reclist, save=True ):\n",
        "  \"\"\"\n",
        "  Subroutine gets the examples from other dictionaries for a given word record(rec)\n",
        "  and appends it to the recordlist(wordlist)\n",
        "  Input:\n",
        "    method\n",
        "    df\n",
        "    file\n",
        "    rec\n",
        "    reclist\n",
        "    save (bool) - true makes file to save every 100 words\n",
        "  \"\"\"\n",
        "  examples=[]\n",
        "  theme = rec[\"theme\"]\n",
        "  parent_word = rec[\"parent_word\"]\n",
        "  lex = rec[\"lex\"]\n",
        "  pos = rec[\"pos\"]\n",
        "  synset = rec[\"synset_name\"]\n",
        "  confidence = rec[\"confidence\"]\n",
        "  words = rec[\"text\"].split(',')\n",
        "  if method=='wordnik':\n",
        "    for word in words:\n",
        "      examples.extend(get_examples_from_wordnik(word))\n",
        "  elif method=='yourdictionary':\n",
        "    for word in words:\n",
        "      examples.extend(get_examples_from_yourdictionary(word))\n",
        "    #examples = [get_examples_from_wordnik(word) for word in words]\n",
        "  for example in examples:\n",
        "    reclist.append({  \n",
        "            \"theme\"          : theme\n",
        "            ,  \"parent_word\" : parent_word\n",
        "            ,  \"synset_name\" : synset\n",
        "            ,  \"confidence\"  : confidence\n",
        "            ,  \"lex\"         : lex\n",
        "            ,  \"pos\"         : pos\n",
        "            ,  \"source\"      : method\n",
        "            ,  \"ttype\"       : 'example'\n",
        "            ,  \"text\"        : example\n",
        "      }) \n",
        "    if len(reclist) > 100:\n",
        "      write_wordlist_to_file(theme, filename, reclist) #write to file at every 100th record\n",
        "      reclist.clear()\n",
        "  return reclist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBZpoXpZrJUh"
      },
      "source": [
        "def extract_more_examples(theme, method, df, option=1):\n",
        "  folder = rootdir + os.sep + datadir \n",
        "  filename = folder + os.sep + theme + '_examples_' + str(option)+'.csv'\n",
        "  #columns=['theme', 'parent_word', 'confidence', 'synset_name', 'lex', 'pos', 'text']\n",
        "  wordlist = []\n",
        "  if option==1:\n",
        "    temp = df[df['ttype']=='lemmas']\n",
        "  else:\n",
        "    temp =  df[ df['ttype']=='related']\n",
        "  #symptoms.progress_apply(lambda row: get_example(method, row, wordlist), axis=1)\n",
        "  df = pd.DataFrame(columns=master_columns)\n",
        "  wordlist = temp.progress_apply(lambda row: get_examples_by_wordrec(method, df, filename, row, wordlist), axis=1)\n",
        "  if len(wordlist) > 0:\n",
        "    #end of processing\n",
        "    print('cleaning up extraction for theme ', themes)\n",
        "    df = write_wordlist_to_file(df, filename, reclist) #write to file\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5PujWs1akRX"
      },
      "source": [
        "def extract_examples(scheme, method, df):\n",
        "  folder = rootdir + os.sep + datadir\n",
        "  filename = folder + os.sep +'wn_themes_' + scheme + '_' + method+ '.csv'\n",
        "  themes = df['theme'].unique()\n",
        "  print('Extracting examples for words via ', method)\n",
        "  for theme in themes:\n",
        "    print('---|Processing theme:', theme)\n",
        "    print('------|Lemmas')\n",
        "    ndf = df[df['theme']== theme]\n",
        "    ndf = ndf.append(extract_more_examples(theme, method, ndf, 1 ))\n",
        "    ndf.to_csv(filename)\n",
        "    print('--------|Checkpoint reached. DF saved to ,', filename)\n",
        "    print('------|Related Words')\n",
        "    ndf = ndf.append(extract_more_examples(theme, method, ndf, 2 ))\n",
        "    print('--------|Checkpoint reached. DF saved to ,', filename)\n",
        "    ndf.to_csv(filename)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKWhYkKopQ-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "681d3213-7f44-47cd-a997-a81f01805b02"
      },
      "source": [
        "\"\"\"def extract_more_examples_and_save(theme, method, df, option=1):\n",
        "  print('.....extracting more examples for theme ', theme, ' using method ', method, 'option:', str(option)) \n",
        "  return extract_more_examples(theme, method, df, option) \"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"def extract_more_examples_and_save(theme, method, df, option=1):\\n  print('.....extracting more examples for theme ', theme, ' using method ', method, 'option:', str(option)) \\n  return extract_more_examples(theme, method, df, option) \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzkiB1oe42xi"
      },
      "source": [
        "##### Methods for extraction from WordNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNu-ILxx4oLh"
      },
      "source": [
        "def extract_syn(synset, word, theme, confidence=0.50,):\n",
        "  \"\"\"\n",
        "  Method to extract data from synset and return in a specified format\n",
        "  INPUT : \n",
        "    synset = synset\n",
        "    word = the parent word\n",
        "    theme = theme\n",
        "    confidence = the confidence level to be used when removing duplicate texts\n",
        "  OUTPUT: extract = the dictionary containing all required info about a word\n",
        "  \"\"\"\n",
        "  more_words = []\n",
        "  for lemma in synset.lemmas(lang='eng'):\n",
        "    related_forms = lemma.derivationally_related_forms()\n",
        "    for forms in related_forms:\n",
        "      if forms.name() not in more_words:\n",
        "        more_words.append(forms.name())\n",
        "    \"\"\"pertainyms = lemma.pertainyms()\n",
        "    for forms in pertainyms:\n",
        "      if forms.name() not in more_words:\n",
        "        more_words.append(forms.name())\"\"\"\n",
        "  extract = { \n",
        "            \"theme\": theme,\n",
        "            \"parent_word\": word, \n",
        "            \"confidence\": confidence,\n",
        "            \"words\": synset.lemma_names(lang='eng'),\n",
        "            \"synset\": synset.name(),\n",
        "            \"lex\": synset.lexname(),\n",
        "            \"pos\": synset.pos(),\n",
        "            \"domain\": synset.topic_domains(),\n",
        "            \"definition\":synset.definition(),\n",
        "            \"examples\": synset.examples(),\n",
        "            \"related_words\" : more_words\n",
        "          }\n",
        "  return extract"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OIHsIA9DXAO"
      },
      "source": [
        "def append_rec_to_list( rec=wordrec, reclist=wordlist):\n",
        "  \"\"\"\n",
        "  Method to add a word record to record list\n",
        "  INPUT: \n",
        "    rec = formatted extract (word record)\n",
        "    reclist = the list\n",
        "  \"\"\"\n",
        "  words = rec.get(\"words\")\n",
        "  word_count = len(words)\n",
        "  if word_count > 1 :\n",
        "    words = ','.join(words)\n",
        "  elif word_count == 1:\n",
        "    words = words[0]\n",
        "  else:\n",
        "    words = ''    \n",
        "  related_words = rec.get(\"related_words\")\n",
        "  word_count = len(related_words)\n",
        "  if word_count > 1 :\n",
        "    related_words = ','.join(related_words)\n",
        "  elif word_count == 1:\n",
        "    related_words = related_words[0]\n",
        "  else:\n",
        "    related_words = '' \n",
        "  \n",
        "  theme = rec.get(\"theme\")\n",
        "  parent_word = rec.get(\"parent_word\")\n",
        "  lex = rec.get(\"lex\")\n",
        "  pos = rec.get(\"pos\")\n",
        "  synset = rec.get(\"synset\")\n",
        "  confidence = rec[\"confidence\"]\n",
        "# append definition\n",
        "  definition = rec.get(\"definition\")\n",
        "  if len(definition) > 0: \n",
        "    reclist.append({\n",
        "              \"theme\"       :  theme\n",
        "           ,  \"parent_word\" : parent_word\n",
        "           ,  \"confidence\"  : confidence\n",
        "           ,  \"synset_name\" : synset\n",
        "           ,  \"lex\"         : lex\n",
        "           ,  \"pos\"         : pos\n",
        "           ,  \"source\"      : 'wordnet'\n",
        "           ,  \"ttype\"       : 'definition'\n",
        "           ,  \"text\"        : definition\n",
        "    })\n",
        "# append the words as text\n",
        "  if len(words) > 0: \n",
        "    reclist.append({  \n",
        "              \"theme\"       :  theme\n",
        "           ,  \"parent_word\" : parent_word\n",
        "           ,  \"confidence\"  : confidence\n",
        "           ,  \"synset_name\" : synset\n",
        "           ,  \"lex\"         : lex\n",
        "           ,  \"pos\"         : pos\n",
        "           ,  \"source\"      : 'wordnet'\n",
        "           ,  \"ttype\"       : 'lemmas'\n",
        "           ,  \"text\"        : words\n",
        "    })\n",
        "\n",
        "# append the related words as text\n",
        "  if len(related_words) > 0: \n",
        "    reclist.append({                \n",
        "            \"theme\"         :  theme\n",
        "           ,  \"parent_word\" : parent_word\n",
        "           ,  \"confidence\"  : confidence\n",
        "           ,  \"synset_name\" : synset\n",
        "           ,  \"lex\"         : lex\n",
        "           ,  \"pos\"         : pos\n",
        "           ,  \"source\"      : 'wordnet' \n",
        "           ,  \"ttype\"       : 'related'\n",
        "           ,  \"text\"        : related_words\n",
        "    })\n",
        "        \n",
        "  examples = rec.get(\"examples\")\n",
        "  if len(examples) !=0 :\n",
        "    for example in examples:\n",
        "      reclist.append({  \n",
        "            \"theme\" :  theme\n",
        "           ,  \"parent_word\" : parent_word\n",
        "           ,  \"synset_name\" : synset\n",
        "           ,  \"confidence\"  : confidence\n",
        "           ,  \"lex\"         : lex\n",
        "           ,  \"pos\"         : pos\n",
        "           ,  \"source\"      : 'wordnet'\n",
        "           ,  \"ttype\"       : 'example'\n",
        "           ,  \"text\"        : example\n",
        "      })\n",
        "  else:\n",
        "    example = ''\n",
        "    reclist.append({  \n",
        "          \"theme\" :  theme\n",
        "          ,  \"parent_word\" : parent_word\n",
        "          ,  \"synset_name\" : synset\n",
        "          ,  \"confidence\"  : confidence\n",
        "          ,  \"lex\"         : lex\n",
        "          ,  \"pos\"         : pos\n",
        "          ,  \"source\"      : 'wordnet'\n",
        "          ,  \"ttype\"       : 'example'\n",
        "          ,  \"text\"        : example\n",
        "    })    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xotgu_qhWMfa"
      },
      "source": [
        "def get_hyponyms(synset):\n",
        "  \"\"\"\n",
        "  iterative function to get the hyponyms of a synset\n",
        "  input: sysnet\n",
        "  output: list of hyponyms up to last level in hierarchy\n",
        "  \"\"\"\n",
        "  list1 = synset.hyponyms( )\n",
        "  if len(list1)!=0:\n",
        "    list2 = []\n",
        "    for item in list1:\n",
        "      list3 = get_hyponyms(item)\n",
        "      if len(list3)!=0: \n",
        "        list2.extend(list3)\n",
        "    if len(list2)!=0:    \n",
        "      list1.extend(list2)\n",
        "  return list1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veKy9DiE7BAN"
      },
      "source": [
        "def get_list_for_word(theme, word, wordlist,):\n",
        "  \"\"\"\n",
        "  extract synsets, examples, definitions, related wirds for a given word\n",
        "  and append it to the wordlist\n",
        "  INPUT:theme \n",
        "        word\n",
        "        wordlist\n",
        "  \"\"\"\n",
        "  if len(word) > 0:\n",
        "    #print('word is :' , word)\n",
        "    synsets = wn.synsets(word, lang='eng')\n",
        "    related_words = []\n",
        "    for synset in synsets:\n",
        "      word_ = synset.name().rsplit('.')[0]\n",
        "      if word == word_ :\n",
        "        confidence = 1.00\n",
        "      else: confidence = 0.70\n",
        "      wordrec = extract_syn(synset, word_, theme, confidence )\n",
        "      related_words = wordrec['related_words']\n",
        "      append_rec_to_list( wordrec, wordlist)\n",
        "      hyponyms = get_hyponyms(synset)\n",
        "      for hyponym in hyponyms:\n",
        "        wordrec = extract_syn(hyponym, hyponym.name().rsplit('.')[0], theme )\n",
        "        append_rec_to_list( wordrec, wordlist)\n",
        "      similars = synset.similar_tos()\n",
        "      for similar in similars:\n",
        "        wordrec = extract_syn(similar, similar.name().rsplit('.')[0], theme)\n",
        "        append_rec_to_list( wordrec, wordlist)\n",
        "        #[get_list_for_word(theme, word_, wordlist) for word_ in related_words if ( word_!=word and len(word_)>0 )]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KcFULBvGYwQ"
      },
      "source": [
        "def get_list_for_theme(theme, wordSet, wordlist):\n",
        "  \"\"\"\n",
        "  Extract a list of sentences for a set of words in a theme\n",
        "  and append it to the wordlist\n",
        "  INPUT: \n",
        "    theme(str)\n",
        "    wordSet(str) - words separated by comma\n",
        "    wordList(list)- the sentences list(global param)\n",
        "  \"\"\"\n",
        "  _wordSet = wordSet.split(',')\n",
        "  for _word in _wordSet:\n",
        "      get_list_for_word(theme, _word, wordlist,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw2GisxaLW7u"
      },
      "source": [
        "def load_seed_words( scheme ):\n",
        "  \"\"\"\n",
        "  Load the seed words from file\n",
        "  input: \n",
        "      scheme(int) - the scheme determines the file to load\n",
        "                    if you have only one scheme, use 1.\n",
        "   file structure:\n",
        "        theme, [seed_word1, seedword2, seedword3,...,seedwordn]\n",
        "\n",
        "  \"\"\"\n",
        "  folder = rootdir + os.sep + 'Data'\n",
        "  filename = folder + os.sep +'seedwords_' + scheme + '.csv'\n",
        "  return pd.read_csv(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRq8eOQCcF6X"
      },
      "source": [
        "def main( scheme ):\n",
        "  \"\"\"\n",
        "  the main routine\n",
        "  input :\n",
        "    scheme(int) - the scheme determines the file to load\n",
        "                  if you have only one scheme, use 1.\n",
        "  \"\"\"\n",
        "  columns=['theme', 'parent_word', 'confidence', 'synset_name', 'lex', 'pos', 'text']\n",
        "  df = pd.DataFrame(columns=columns)\n",
        "  wordset = set()\n",
        "  wordrec = { \"theme\": '',\n",
        "              \"parent_word\": '', \n",
        "              \"words\": [],\n",
        "              \"synset\": '',\n",
        "              \"lex\": '',\n",
        "              \"pos\": '',\n",
        "              \"domain\": [],\n",
        "              \"definition\":'',\n",
        "              \"examples\": [],\n",
        "              \"similar_to\": []\n",
        "            }\n",
        "  wordlist = []\n",
        "  folder = rootdir + os.sep + datadir\n",
        "  filename = folder + os.sep +'wn_themes_' + scheme + '.csv'\n",
        "  wordlist = []\n",
        "  themes = load_seed_words( scheme )\n",
        "  themes = themes.dropna()\n",
        "  print('Seedwords loaded.')\n",
        "  print('Word count = ', len(themes))\n",
        "  print('Theme count = ', themes['theme'].nunique())\n",
        "  print('Starting WordNet extraction....')\n",
        "  start = timeit.timeit()\n",
        "  # generate the wordlist\n",
        "  themes[['theme', 'words']].apply(lambda x: get_list_for_theme(*x, wordlist), axis=1)\n",
        "  \"\"\"df['col_3'] = df[['col_1','col_2']].apply(lambda x: f(*x), axis=1)\n",
        "   for theme in themes:\n",
        "    theme_name = theme.get(\"theme\")\n",
        "    for word in theme.get(\"words\"):\n",
        "      get_list_for_word(theme_name, word, wordlist, True)  \"\"\"\n",
        "  \n",
        "  #save to disk\n",
        "  df = df.append(wordlist)\n",
        "  df = df.sort_values(by=['parent_word', 'text', 'confidence' ])\n",
        "  df= df.drop_duplicates(['parent_word', 'text'],keep='last').sort_index()\n",
        "  df = df.sort_values(by=['theme', 'text', 'confidence' ])\n",
        "  df= df.drop_duplicates(['theme', 'text'],keep='last').sort_index()\n",
        "  end = timeit.timeit()\n",
        "  df.to_csv( filename )\n",
        "  print('WordNet Extraction complete. Saved to file ,', filename)\n",
        "  print('Time elapsed :', end-start)\n",
        "  return df "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV4qIe8liv2I"
      },
      "source": [
        "def draw_plot(df):\n",
        "  fig, ax = plt.subplots()\n",
        "  fig.suptitle(\"Text Distribution\", fontsize=12)\n",
        "  df.theme.reset_index().groupby(\"theme\").count().sort_values(by= \n",
        "       \"index\").plot(kind=\"barh\", legend=False, \n",
        "        ax=ax).grid(axis='x')\n",
        "  for p in ax.patches:\n",
        "    width = p.get_width()\n",
        "    if width > 10000:\n",
        "            ax.annotate(str(int(p.get_width())), (p.get_x() + p.get_width(), p.get_y()), xytext=(-2, 4), textcoords='offset points', horizontalalignment='right')\n",
        "    else:\n",
        "          ax.annotate(str(p.get_width()), (p.get_x() + p.get_width(), p.get_y()), xytext=(5, 10), textcoords='offset points')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TyB-iFpz4k1"
      },
      "source": [
        "def read_master_file(master_file):\n",
        "  df=pd.read_csv(master_file)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b71W7rXO6WG"
      },
      "source": [
        "##Extract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FYo4eQgMbZv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bcd477f9-0227-45fd-8dfb-979d64044547"
      },
      "source": [
        "if __name__== \"__main__\":\n",
        "  scheme = 'scheme1' #@param ['scheme1', 'scheme2']\n",
        "  tdf = main(scheme)\n",
        "  tdf = tdf[tdf['theme']!='skip-terms']\n",
        "  draw_plot(tdf)\n",
        "# extracting additional examples\n",
        "  method = 'yourdictionary' #@param ['wordnik', 'yourdictionary']\n",
        "  ndf = extract_examples(scheme, method, tdf)\n",
        "  nfile = rootdir + os.sep + datadir \n",
        "  nfile = nfile + os.sep + scheme + 'enh_extract.csv' \n",
        "  ndf.to_csv(nfile)\n",
        "  draw_plot(ndf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seedwords loaded.\n",
            "Word count =  8\n",
            "Theme count =  8\n",
            "Starting WordNet extraction....\n",
            "WordNet Extraction complete. Saved to file , D:\\HWU\\F21MP\\data\\wn_themes_scheme2.csv\n",
            "Time elapsed : -1.0599998859106563e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdIAAAEVCAYAAABUjWQ6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAvjklEQVR4nO3de5xVdb3/8dcbBETQEAUFEVEIRxhgBNNIJVJQPAhGIoqkgJlHs47204xCK1MPeDmdc0xTKW+louENzOQSNAflaMhlEDCR1DEQD4g3riIMn98few1uhrnBnsve8H4+Hvsx6/Jd3/VeI/WZ73evvZciAjMzM9szDeo7gJmZWS5zITUzM8uAC6mZmVkGXEjNzMwy4EJqZmaWARdSMzOzDLiQmtlOJI2QNL0G+1sqqW+y/AtJj9Rg3z+V9Lua6s9sT7iQmtUTSRvSXtslbU5bH7EH/fWVtLKKNg9J+lzS+uS1RNI4SV8qbRMRj0bEGdU430OSbq6qXUR0jYjCal1E5efb5foi4t8j4tJM+zbLhAupWT2JiOalL+CfwKC0bY/W4qlvi4gDgVbAaOCrwBxJzWryJJL2q8n+zLKVC6lZlpHUQNIYSW9J+lDSHyW1TPbdI+nJtLa3SpqZFMEXgLZpo9q2lZ0nIj6LiFeBwcAhpIoqkkZJeilZlqT/lLRG0qeSXpOUL+kyYARwXXKu55L2xZJ+LOk1YKOk/ZJt/dJOvb+kJ5IR8QJJPdKuJyR1Slt/SNLNFV1f2aliSYOTqeRPJBVKOi5tX7Gka5Nr+DTJsP9u/ucx24ULqVn2+Tfgm8DXgbbAx8Ddyb5rgO5JsTsV+A4wMiI2AmcBq9JGtauqc7KIWA/MAE4tZ/cZQB+gM9ACOB/4MCImAI+SGt02j4hBaccMBwYCLSJiWzl9ngNMAloCjwHPSmpURcYqr09SZ2AicDWp0fafgeckNU5rNgwYABwNdAdGVXZes+pwITXLPv8KjI2IlRGxBfgFMFTSfhGxCfg28CvgEeAHEVHp+6LVtIpUYStrK3AgkAcoIv4eEe9X0dedEbEiIjZXsH9+RDwZEVtJXcf+pKaXM3U+8HxEzEj6vgNoCnytTLZVEfER8BxQUAPntX2cC6lZ9jkKeCaZnvwE+DtQAhwGEBFzgbcBAX+soXMeAXxUdmNEzALuIjUiXi1pgqSDquhrRXX3R8R2YCWpkXem2gLvlul7BalrK/V/acubgOY1cF7bx7mQmmWfFcBZEdEi7bV/RLwHIOlKoAmpUeR1acft0aOcJDUH+gEvlrc/Iu6MiF5AV1JTvD+q4nxV5Tgy7dwNgHakrgVSxe2AtLaH70a/q0j9EVLat5JzvVfFcWYZcSE1yz73ArdIOgpAUitJ5yTLnYGbSU3vXkTqZp+C5LjVwCHpH2WpjKQmknoBz5J6H/bBctp8RdJJyXuYG4HPSI2OS893zB5cXy9J30ru6r0a2AK8kuwrAi6U1FDSAFLvE5eq6vr+CAyUdHqS95qk7//dg4xm1eZCapZ9/huYAkyXtJ5UkTkpKTyPALdGxKKIWA78FPiDpCYR8Qapm23eTqaFK5ouvS7p9yPg98B84GvJDT1lHQT8llShfRf4kNR7jwD3A12Scz27G9c3mdT7mR+T+mPgW8l7mgBXAYOAT0jdFbyj36quLyKWkfoD49fA2qSfQRHx+W5kM9tt8oO9zczM9pxHpGZmZhlwITUzM8uAC6mZmVkGXEjNzMwy4EJqZmaWARdSMzOzDLiQmpmZZcCF1MzMLAMupGZmZhlwITUzM8uAC6mZmVkGXEjNzMwy4EJqZmaWARdSMzOzDLiQmpmZZcCF1MzMLAMupGZmZhnYr74D2J5p0aJFdOrUqb5jVMvGjRtp1qxZfceotlzKm0tZIbfy5lJWyK289Zl1/vz5ayOiVU326UKaow477DDmzZtX3zGqpbCwkL59+9Z3jGrLpby5lBVyK28uZYXcylufWSW9W9N9emrXzMwsAy6kZmZmGXAhNTMzy4ALqZmZWQZ8s5GZmeW0ZcuWcf755+9Yf/vtt/nlL3/Jhx9+yOTJk2nQoAGtW7fmoYceAkBSf2A80Bj4HPhRRMxK9jUG7gL6AtuBsRHxVGXndyE1M7Ocduyxx1JUVARASUkJRxxxBEOGDOHggw/mpptuAuDOO+/kl7/8Zekha4FBEbFKUj4wDTgi2TcWWBMRnSU1AFpWdX4X0hy1eWsJHcY8X98xquWabtsYlSNZIbfy5lJWyK28uZQVcitvJlmLxw+sdP/MmTPp2LEjRx111E7bN27ciCQAImJh2q6lwP6SmkTEFuASIC9pt51U0a2U3yM1M7O9xuOPP87w4cN3rI8dO5YjjzySRx99NH1Emu5cYGFEbJHUItl2k6QFkiZJOqyqc7qQmpnZXuHzzz9nypQpnHfeeTu23XLLLaxYsYIRI0Zw11137dReUlfgVuBfk037Ae2AORHRE3gZuKOq87qQmpnZXuGFF16gZ8+eHHbYroPICy+8kKee+uKeIUntgGeAiyPirWTzh8CmZDvAJKBnVed1ITUzs73CxIkTd5rWXb58+Y7lKVOmkJeXB0Ayhfs88JOImFPaJiICeI7UHbsApwOvV3Ve32xUyyT9AtgAHATMjoi/VNJ2FDA9IlbVTTozs73Dpk2bmDFjBvfdd9+ObWPGjGHZsmU0aNCAo446invvvbd0VPp9oBNwg6QbkuZnRMQa4MfAHyT9F/ABMLqqc7uQ1pGI+Fk1mo0ClgAupGZmu+GAAw7gww8/3Glb+lRuuoi4Gbi5gn3vAn1259ye2q0FksZKWibpL8CxybaHJA1NlntJ+h9J8yVNk9Qm2XcC8KikIklN6/ESzMysmlxIa5ikXsAFwPHAt4CvlNnfCPg1MDQiegEPALdExJPAPGBERBRExOZy+r5M0jxJ8zasW1fbl2JmZtXgqd2adyrwTERsApA0pcz+Y4F8YEby4eCGwPvV6TgiJgATANof0ylqKrCZme05F9LaUVmRE7A0InrXVRgzM6s9ntqtebOBIZKaSjoQGFRm/zKglaTekJrqTT4UDLAeOLDuopqZWaY8Iq1hEbFA0hNAEfAu8GKZ/Z8nNxbdKelLpP4b/Bep73t8CLhX0magd3nvk5qZWXZxIa0FEXELcEsl+4so5/bq5FE9lT6ux8zMsounds3MzDLgQmpmZpYBT+3mqKaNGrKsiufyZYvCwkKKR/St7xjVlkt5cykr5FbeXMoKuZU3l7JWh0ekZmZmGXAhNTMzy4ALqZmZWQZcSM3MzDLgQmpmZpYBF1IzM7MMuJCamZllwIXUzMwsAy6kZmZmGXAhNTMzy4ALqZmZWQZcSM3MzDLgQmq2D/jss8848cQT6dGjB127duXnP/85AB999BH9+/fny1/+Mv379+fjjz8GYOvWrYwcOZJu3bpx3HHHMW7cuB19DRgwYEc/l19+OSUlJfVyTWbZwoXUbB/QpEkTZs2axaJFiygqKmLq1Km88sorjB8/ntNPP53ly5dz+umnM378eAAmTZrEli1bWLx4MfPnz+e+++6juLgYgD/+8Y8sWrSIJUuW8MEHHzBp0qR6vDKz+ufHqOWozVtL6DDm+fqOUS3XdNvGqBzJCrmVt2zW4goerSeJ5s2bA6nR5tatW5HE5MmTKSwsBGDkyJH07duXW2+9FUls3LiRbdu2sXnzZho3bsxBBx0EsOPntm3b+Pzzz5FUi1dolv08IjXbR5SUlFBQUEDr1q3p378/J510EqtXr6ZNmzYAtGnThjVr1gAwdOhQmjVrRps2bWjfvj3XXnstLVu23NHXmWeeSevWrTnwwAMZOnRovVyPWbZwITXbRzRs2JCioiJWrlzJ3LlzWbJkSYVt586dS8OGDVm1ahXvvPMO//Ef/8Hbb7+9Y/+0adN4//332bJlC7NmzaqL+GZZy4XUbB/TokUL+vbty9SpUznssMN4//33AXj//fdp3bo1AI899hgDBgygUaNGtG7dmpNPPpl58+bt1M/+++/P4MGDmTx5cp1fg1k2cSE12wd88MEHfPLJJwBs3ryZv/zlL+Tl5TF48GAefvhhAB5++GHOOeccANq3b8+sWbOICDZu3Mgrr7xCXl4eGzZs2FF4t23bxp///Gfy8vLq5ZrMsoVvNqpFkvoC10bE2ZW0+QWwISLuqKNYtg96//33GTlyJCUlJWzfvp1hw4Zx9tln07t3b4YNG8b9999P+/btd9yBe+WVVzJ69Gjy8/OJCEaPHk337t1ZvXo1gwcPZsuWLZSUlHDaaadx+eWX1/PVmdUvF1KzfUD37t1ZuHDhLtsPOeQQZs6cucv25s2bl/uxlsMOO4xXX321VjKa5SpP7WZAUjNJz0taJGmJpPMlDZD0hqSXgG+ltW0p6VlJr0l6RVL3tK66SCqU9Lakf6v7KzEzsz3lEWlmBgCrImIggKQvAUuA04B/AE+ktb0RWBgR35R0GvB7oCDZlwd8AzgQWCbpnojYWvZkki4DLgM4+JBWHFQrl2RmZrvDI9LMLAb6SbpV0qnA0cA7EbE8IgJ4JK3tKcAfACJiFnBIUngBno+ILRGxFlgDHFbeySJiQkScEBEnND/IZdTMLBu4kGYgIt4EepEqqOOAwUBU0Ly8r38pbbslbVsJnikwM8sZLqQZkNQW2BQRjwB3AF8DjpbUMWkyPK35bGBEclxfYG1ErKu7tGZmVhs88slMN+B2SduBrcAVwKHA85LWAi8B+UnbXwAPSnoN2ASMrPu4ZmZW01xIMxAR04Bp5eza5RPqEfERcE45239RZj2/bBszM8tento1MzPLgAupmZlZBjy1m6OaNmrIsgqePZltCgsLKR7Rt75jVFsu5c2lrGZ7K49IzczMMuBCamZmlgEXUjMzswy4kJqZmWXAhdTMzCwDLqRmZmYZcCE1MzPLgAupmZlZBlxIzczMMuBCamZmlgEXUjMzswy4kJqZmWXAhdSsApdccgmtW7cmP/+LR8QuWrSI3r17061bNwYNGsS6det27Bs3bhydOnXi2GOPZdq0Lx5T+8QTT9C9e3e6du3KddddV6fXYGa1z4XUrAKjRo1i6tSpO2279NJLGT9+PIsXL2bIkCHcfvvtALz++us8/vjjLF26lKlTp/K9732PkpISPvzwQ370ox8xc+ZMli5dyurVq5k5c2Z9XI6Z1RI/Ri1Hbd5aQocxz9d3jGq5pts2RmVx1uIKHkfXp08fiouLd9q2bNky+vTpA0D//v0588wzuemmm5g8eTIXXHABTZo04eijj6ZTp07MnTuX/fbbj86dO9OqVSsA+vXrx1NPPcXpp59eq9dkZnXHI1Kz3ZCfn8+UKVMAmDRpEitWrADgvffe48gjj9zRrl27drz33nt06tSJN954g+LiYrZt28azzz674xgz2zu4kJrthgceeIC7776bXr16sX79eho3bgxAROzSVhIHH3ww99xzD+effz6nnnoqHTp0YL/9PBFktjfx/6LNdkNeXh7Tp08H4M033+T551NT1u3atdtppLly5Uratm0LwKBBgxg0aBAAEyZMoGHDhnWc2sxqk0ekZrthzZo1AGzfvp2bb76Zyy+/HIDBgwfz+OOPs2XLFt555x2WL1/OiSeeuNMxH3/8Mb/5zW+49NJL6ye8mdWKOi+kkoolHZphHydIunMPjy2UdEIm50/6GSWpbdr67yR1qeKY/01+dpB0YaYZrHYNHz6c3r17s2zZMtq1a8f999/PxIkT6dy5M3l5ebRt25bRo0cD0LVrV4YNG0aXLl0YMGAAd999946R51VXXUWXLl04+eSTGTNmDJ07d67PyzKzGpaTU7sRMQ+YV88xRgFLgFUAEVHlMCMivpYsdgAuBB6rpWxWAyZOnFju9quuuqrc7WPHjmXs2LHV7sfM9g61NiJNRl1vSHpY0muSnpR0QLL7B5IWSFosKU9SA0nLJbVKjm0g6R+SDpV0nqQlkhZJmp3s7yvpT8lyc0kPJn29JuncZPs9kuZJWirpxmrmfTHJtUDS19L2XZf0v0jSeElDgROARyUVSWpaOtKVdIWk29KOHSXp18nyhmTzeODU5NgfJuctSDtmjqTue/7bNzOzulLbU7vHAhMiojuwDvhesn1tRPQE7gGujYjtwCPAiGR/P2BRRKwFfgacGRE9gMHlnOMG4NOI6JacZ1ayfWxEnAB0B75ejcK0Buif5DofuBNA0lnAN4GTkgy3RcSTpEbEIyKiICI2p/XzJPCttPXzgSfKnGsM8GJy7H8CvyM1wkVSZ6BJRLxWNqCky5I/DuZtSPtGHTMzqz+1XUhXRMScZPkR4JRk+enk53xS05wADwAXJ8uXAA8my3OAhyR9Fyjvdsd+wN2lKxHxcbI4TNICYCHQFaj0/UugEfBbSYuBSWnt+wEPRsSmpP+PKuskIj4A3pb0VUmHkPpjYk5lxyTnO1tSI1LX/lAFfU+IiBMi4oTmBx1URZdmZlYXavs90rIfritd35L8LCnNEBErJK2WdBpwEsnoNCIul3QSMBAoSp8CTajseSQdDVwLfCUiPpb0ELB/mTZDgJ8nq5cCZwOrgR6k/sD4rKL+q+EJYBjwBvBMlPchwzQRsUnSDOCc5LiMb4YyM7O6Udsj0vaSeifLw4GXqmj/O1Ij1z9GRAmApI4R8beI+BmwFjiyzDHTge+Xrkg6GDgI2Ah8Kukw4KyyJ4qIZ5Kp1YLk5qUvAe8n08wX8cXodzpwSen7u5JaJtvXAwdWcB1Pk5oOHs6u07oVHfs7UtPJr1Y16jUzs+xR24X078BISa8BLUm9J1qZKUBzvpjWBbg9udFnCTAbWFTmmJuBg0tvSAK+ERGLSE3pLiU1ZVzV1CrAb5KsrwCdSRViImJqkmuepCJSI11ITb/eW3qzUXpHyfTy68BRETG3nHO9BmxLbl76YXLMfFLvIz9YTnszM8tStT21uz0iLi+zrUPpQjIS7Ju2rwepm4zeSGuTfuNOqcLkRURsAEaWbRARo8oLFBF9K9i+nNSNSaV+krZvPKk7bdPbPwU8lbapb5n9Z5dzjubJz63ATt9annwmtQGpEbCZmeWIrPlmI0ljSBWmn1TVdm8j6WLgb6TuNN5e33nMzKz6am1EGhHFQH5V7dLa7zLq21dExO+B39d3DjMz2305+c1GBk0bNWRZBc/RzDaFhYUUj+hb3zHMzGpF1kztmpmZ5SIXUjMzswy4kJqZmWXAhdTMzCwDLqRmZmYZcCE1MzPLgAupmZlZBlxIzczMMuBCamZmloEqC6lSvi3pZ8l6e0kn1n40MzOz7FedEelvgN6knq0JqWdp3l1riczMzHJIdb5r96SI6ClpIaSetSmpcS3nMjMzywnVGZFuldQQCABJrQA/6suyyiWXXELr1q3Jz9/1gUN33HEHkli7di0AM2bMoFevXnTr1o1evXoxa9asXY4ZPHhwuX2ZmZVVnUJ6J/AM0FrSLcBLwL/Xaiqz3TRq1CimTp26y/YVK1YwY8YM2rdvv2PboYceynPPPcfixYt5+OGHueiii3Y6Zvbs2TRv3rzWM5vZ3qHKqd2IeFTSfOB0QMA3I+LvtZ7MKrV5awkdxjxf3zGq5Zpu2xhVQ1mLK3h0XJ8+fSguLt5l+w9/+ENuu+02zjnnnB3bjj/++B3LXbt25bPPPmPLli00adKEDRs2MGnSJJ544gmGDRtWI5nNbO9W3eeRrgZeTNo3ldQzIhbUXiyzzE2ZMoUjjjiCHj16VNjmqaee4vjjj6dJkyYA3HDDDQwbNowDDjigrmKaWY6rspBKugkYBbxF8j5p8vO02otllplNmzZxyy23MH369ArbLF26lB//+Mc72hQVFfGPf/yDa665pq5imtleoDoj0mFAx4j4vLbDmNWUt956i3feeWfHaHTlypX07NmTuXPncvjhh7Ny5UqGDBnC73//ezp27AjAyy+/zPz587ngggvYb7/9WLNmDX379qWwsLAer8TMsl11bjZaArSo5RxmNapbt26sWbOG4uJiiouLadeuHQsWLODwww/nk08+YeDAgYwbN46TTz55xzFXXHEFq1at4vHHH+ell16ic+fOLqJmVqXqFNJxwEJJ0yRNKX3VdrA9JenPklpUsr9Y0qFV9NFE0l8kFUk6fzfP30HShbtzjGVu+PDh9O7dm2XLltGuXTvuv//+Ctvedddd/OMf/+Cmm26ioKCAgoIC1qxZU4dpzWxvUp2p3YeBW4HFZPnnRyUJODsiMs15PNAoIgr24NgOwIXAY9U9QNJ+EbFtD85liYkTJ1a6P/2O3uuvv57rr7++0vYdOnRgyZIlNRHNzPZy1RmRro2IOyPirxHxP6WvWk9WTckI8O+SfgMsAEokHSqpmaTnJS2StKTsyFJSU0lTJX23zPbWwCNAQTIi7SjpZ5JeTfqZkBRsJHVKRq6LJC2Q1BEYD5yaHPtDSftLelDSYkkLJX0jOXaUpEmSngOmS2ojaXZy3BJJp9bBr8/MzDJUnUI6X9I4Sb0l9Sx91Xqy3XMs8PuIOB54N9k2AFgVET0iIh9I/7R+c+A54LGI+G16RxGxBrgUeDEiCiLiLeCuiPhK0k9T4Oyk+aPA3RHRA/ga8D4wJu3Y/wSuTPrtRur7ih+WtH9yfG9gZEScRmoUOy0ZBfcAispepKTLJM2TNG/DunV79psyM7MaVZ2p3dJPr381bVu2ffzl3Yh4pcy2xcAdkm4F/hQRL6btmwzcFhGPVrP/b0i6DjgAaAkslVQIHBERzwBExGcAyWA13SnAr5M2b0h6F+ic7JsRER8ly68CD0hqBDwbEUVlO4qICcAEgPbHdIqy+83MrO5VOSKNiG+U88qmIgqwseyGiHgT6EWqoI4rfQxcYg5wVtoU7ZXJlGqRpLbp/SSjx98AQ5NR5W+B/Ul9y1N1VNZuR+6ImA30Ad4D/iDp4mr2b2Zm9ag6zyM9TNL9kl5I1rtI+k7tR8tMUhA3RcQjwB1A+nT0z4APSRVIIuLuZCq2ICJWlemqdBp2raTmwNDkmHXASknfTM7XRNIBpB4zd2Da8bOBEUmbzkB7YFk5eY8C1iRTzfeXyWtmZlmqOu+RPgRMA0pHam8CV9dSnprUDZgrqQgYC9xcZv/VwP6Sbqusk4j4hNQodDHwLKkp2FIXAf8m6TXgf4HDgdeAbckNSD8kVawbSloMPAGMiogt5ZyqL1CUPK7uXOC/q3uhZmZWf6rzHumhEfFHST8BiIhtkkpqOVe1RUQxkJ+23iFZnJa8yrbvkLY6uoI+C4HCtPXrgV0+LxERyyn/veLTy6yPKufYh0j9kVK6/jCpjxqZmVkOqc6IdKOkQ/jieaRfBT6t1VRmZmY5ojoj0v8HTAE6SpoDtCJ5n9DMzGxfV53nkS6Q9HVSn9UUsCwittZ6MqtU00YNWVbBszmzTWFhIcUj+tZ3DDOzWlHd55GeSOqr7/YDekoiIn5fa6nMzMxyRHWeR/oHoCOpb9opvckoABdSMzPb51VnRHoC0CUi/E06ZmZmZVT3eaSH13YQMzOzXFThiDR5KkmQ+pae1yXNBXZ8kUBEDK79eGZmZtmtsqndO0jdpXsr8M207aXbzMzM9nkVFtLSZ45KalT2+aOSmtZ2MDMzs1xQ2dTuFcD3gGOS75ItdSCpp6eYmZnt8yqb2n0MeAEYR+ph1aXWpz1D08zMbJ9W2dTup6S+U3d43cUxMzPLLdX5+IuZmZlVwIXUzMwsAy6kZmZmGXAhtaxyySWX0Lp1a/LzdzyrnUmTJtG1a1caNGjAvHnzdmo/btw4OnXqxLHHHsu0aV88x/3zzz/nsssuo3PnzuTl5fHUU0/V2TWY2b6luk9/sSyzeWsJHcY8X98xquWabtsYVSZrcQWPgBs1ahTf//73ufjii3dsy8/P5+mnn+Zf//Vfd2r7+uuv8/jjj7N06VJWrVpFv379ePPNN2nYsCG33HILrVu35s0332T79u189JFvNDez2uFCalmlT58+FBcX77TtuOOOK7ft5MmTueCCC2jSpAlHH300nTp1Yu7cufTu3ZsHHniAN954A4AGDRpw6KGH1nZ0M9tHeWrXctZ7773HkUceuWO9Xbt2vPfee3zyyScA3HDDDfTs2ZPzzjuP1atX11NKM9vbuZBazirvyX6S2LZtGytXruTkk09mwYIF9O7dm2uvvbYeEprZvsCF1HJWu3btWLFixY71lStX0rZtWw455BAOOOAAhgwZAsB5553HggUL6iumme3lXEh3k6RCSSfUdw6DwYMH8/jjj7Nlyxbeeecdli9fzoknnogkBg0aRGFhIQAzZ86kS5cu9RvWzPZa+8TNRpIaRkRJfeewqg0fPpzCwkLWrl1Lu3btuPHGG2nZsiU/+MEP+OCDDxg4cCAFBQVMmzaNrl27MmzYMLp06cJ+++3H3XffTcOGDQG49dZbueiii7j66qtp1aoVDz74YD1fmZntrXK+kErqAEwF/gYcD7wJXAy8DjwAnAHcJekj4EagCfAWMBo4FRgdEcOSvvoC10TEIEn3AF8BmgJPRsTPyzn3GWX7jIgNkoqBh4FBQCPgvIh4Q1Jz4NfACaQemn5jRDxVUT81+GvKGRMnTix3e+k0bVljx45l7Nixu2w/6qijmD17do1mMzMrz94ytXssMCEiugPrSD3+DeCziDgF+AtwPdAvInoC84D/B8wAviqpWdL+fOCJZHlsRJwAdAe+Lql7+gklHVpBn6XWJtvvAUrvdLkB+DQiuiVZZ1Wjn/RzXiZpnqR5G9at293fkZmZ1YKcH5EmVkRE6TNSHwH+LVkuLYpfBboAcyQBNAZejohtkqYCgyQ9CQwErkuOGSbpMlK/ozbJ8enPZS23z7T9Tyc/5wPfSpb7AReUNoiIjyWdXUU/pLWfAEwAaH9Mp11vWTUzszq3txTSskWldH1j8lPAjIgo75FwTwBXAh8Br0bEeklHkxpFfiUpdg8B+5c5rrI+AbYkP0v44vescrJW1Y+ZmWWxvWVqt72k3snycOClMvtfAU6W1AlA0gGSOif7CoGewHf5YgR7EKki/Kmkw4CzyjlnZX1WZDrw/dIVSQfvYT9mZpYl9pZC+ndgpKTXgJak3pfcISI+AEYBE5M2rwB5yb4S4E+kiuWfkm2LgIXAUlI3LM2hjMr6rMTNwMGSlkhaBHxjD/sxM7MssbdM7W6PiMvLbOuQvhIRs0jdhbuLiPg+aSPFZNuoCtr2rarPiOiQtjwP6JssbwBGltO+wmxmZpbd9pYRqZmZWb3I+RFpRBQD+VW1MzMzqw05X0j3VU0bNWRZBc/0zDaFhYUUj+hb3zHMzGqFp3bNzMwy4EJqZmaWARdSMzOzDLiQmpmZZcCF1MzMLAMupGZmZhlwITUzM8uAC6mZmVkGXEjNzMwy4EJqZmaWARdSMzOzDLiQmpmZZcCF1MzMLAMupFZjPvnkE4YOHUpeXh7HHXccL7/8MosWLeLKK6+kW7duDBo0iHXr1gFQXFxM06ZNKSgooKCggMsvL/tcdjOz3ODHqOWozVtL6DDm+To/b3Elj2676qqrGDBgAE8++SSff/45mzZton///nz3u9/l6quv5oEHHuD222/npptuAqBjx44UFRXVUXIzs9rhEanViHXr1jF79my+853vANC4cWNatGjBsmXL6NGjBwD9+/fnqaeeqs+YZmY1zoXUasTbb79Nq1atGD16NMcffzyXXnopGzduJD8/nzlz5gAwadIkVqxYseOYd955h+OPP56vf/3rvPjii/UV3cwsIy6kViO2bdvGggULuOKKK1i4cCHNmjVj/PjxPPDAA0yePJlevXqxfv16GjduDECbNm345z//ycKFC/nVr37FhRdeuOP9UzOzXOJCajWiXbt2tGvXjpNOOgmAoUOHsmDBAvLy8rj99tuZP38+w4cPp2PHjgA0adKEQw45BIBevXrRsWNH3nzzzXrLb2a2p3K2kEraUMX+FpK+twf9/kLStTXV377i8MMP58gjj2TZsmUAzJw5ky5durBmzRoAtm/fzs0337zj7twPPviAkpISIDUtvHz5co455pj6CW9mloGsLqRK2dOMLYCaLHy73V+G+XPOr3/9a0aMGEH37t0pKiripz/9KRMnTuSiiy4iLy+Ptm3bMnr0aABmz55N9+7d6dGjB0OHDuXee++lZcuW9XwFZma7L+s+/iKpA/AC8FegN/CspLOBJsAzEfHzMu2bA5OBg4FGwPURMRkYD3SUVATMiIgfSfoRMKxsX5LGAhcDK4APgPnlRKtWf+Xkv1rSfcBLwFeBRcCDwI1Aa2BERMyV9HXgv5NzBdAnItbv2W+xfhQUFDBv3rydtl111VX06NGDvn377rT93HPP5dxzz63DdGZmtSPrCmniWGA08CwwFDgREDBFUp+ImJ3W9jNgSESsk3Qo8IqkKcAYID8iCgAknQF8uWxfwEbgAuB4Ur+PBZRfSKvb3z9L80fE95LC2gk4D7gMeBW4EDgFGAz8FPgmcC1wZUTMSf44+KxsAEmXJX1w8CGtOKh6v0szM6tF2VpI342IVyTdAZwBLEy2NydVvNILqYB/T4rYduAI4LBy+jyjgr4OJDWa3ASQFOHqqKi/f5bmT2v7TkQsTvpfCsyMiJC0GOiQtJkD/ErSo8DTEbGy7AkjYgIwAaD9MZ2imjnNzKwWZWsh3Zj8FDAuIu6rpO0IoBXQKyK2SioG9i+nXbl9Sbqa1FQqZbYfCTyXrN4LTK1mfx3S8pfakra8PW19O8l/g4gYL+l54F9Ijar7RcQb5VyHmZllkWy/EWYacEky1YmkIyS1LtPmS8CapIh+Azgq2b6e1Gizqr5mA0MkNZV0IDAIICJWRERB8rp3N/rbI5I6RsTiiLgVmAfk7WlfZmZWd7J1RApAREyXdBzwsiSADcC3gTVpzR4FnpM0DygC3kiO/VDSHElLgBeSm4N26SsiFkh6Ijn2XaDcr9ipbn9AyR5e7tXJHwIlwOukblgyM7Msl3WFNCKKgfy09f/mi7tZ09s1T36uJXV3bHl9XVhmvaK+bgFuqUa2avVXJn9xmfVR5e2LiB9UdX4zM8s+2T61a2ZmltVcSM3MzDKQdVO7Vj1NGzVkWSXPBjUzs7rhEamZmVkGXEjNzMwy4EJqZmaWARdSMzOzDLiQmpmZZcCF1MzMLAMupGZmZhlwITUzM8uAC6mZmVkGXEjNzMwy4EJqZmaWARdSMzOzDLiQmpmZZcCF1KqtQ4cOdOvWjYKCAk444QQAJk2aRNeuXWnQoAHz5s3b0Xbr1q2MHDmSbt26MXLkSMaNG1dfsc3MapULqe2Wv/71rxQVFe0omvn5+Tz99NP06dNnp3aTJk1iy5YtLF68mPvuu4/77ruP4uLiekhsZla7/DzSHLV5awkdxjxf4/0W7+YzTo877rhyt0ti48aNbNu2jS1bttC4cWMOOuigmohoZpZVPCK1apPEGWecQa9evZgwYUKlbYcOHUqzZs1o06YNF1xwAddeey0tW7aso6RmZnXHI1Krtjlz5tC2bVvWrFlD//79ycvL22VKt9TcuXNp2LAhq1at4k9/+hNjxoyhX79+HHPMMXWc2sysdnlEatXWtm1bAFq3bs2QIUOYO3duhW0fe+wxBgwYQKNGjTj44IM5+eSTd7oZycxsb+FCuocktZD0vfrOUVc2btzI+vXrdyxPnz6d/Pz8Ctu3b9+eWbNmERFs3ryZV155hby8vLqKa2ZWZ1xI91wLYJ8ppKtXr+aUU06hR48enHjiiQwcOJABAwbwzDPP0K5dO15++WUGDhzImWeeCcCVV17Jhg0byM/P54orrmD06NF07969nq/CzKzm7XXvkUpqBvwRaAc0BG4CLoiIIcn+/sAVEfEtSRuAu4F+wMfAT4HbgPbA1RExRdIoYAjQBDgaeCwibgTGAx0lFQEzgOuSY88CArg5Ip6Q1Be4EVgNFABPA4uBq4CmwDcj4i1J5wE/B0qATyOi/Dcf68kxxxzDokWLdtk+ZMgQhgwZssv25s2bM2nSJAAKCwvp27dvbUc0M6sXe10hBQYAqyJiIICkLwE3SmoVER8Ao4EHk7bNgMKI+LGkZ4Cbgf5AF+BhYErS7kQgH9gEvCrpeWAMkB8RBcl5ziVVKHsAhybtZifH9wCOAz4C3gZ+FxEnSroK+AFwNfAz4MyIeE9Si/IuTNJlwGUABx/SCn+YxMys/u2NU7uLgX6SbpV0akR8CvwB+HZSoHoDLyRtPwemph33PxGxNVnukNbnjIj4MCI2kxpRnlLOeU8BJkZESUSsBv4H+Eqy79WIeD8itgBvAdPTzll6njnAQ5K+S2okvYuImBARJ0TECc39mUwzs6yw141II+JNSb2AfwHGSZoO/A54DvgMmBQR25LmWyMikuXtwJakj+2S0n83wc7KrgOoklhb0pa3p61vJ/lvEBGXSzoJGAgUSSqIiA8r6dPMzLLAXjcildQW2BQRjwB3AD0jYhWwCrgeeGgPuu0vqaWkpsA3SY0e1wMHprWZDZwvqaGkVkAfoOLPh+yau2NE/C0ifgasBY7cg5xmZlbH9roRKdANuF3SdmArcEWy/VGgVUS8vgd9vkRqergTqZuN5gFImiNpCamp4utITRsvIjVivS4i/k9SdT/zcbukL5Ma2c5M+jEzsyy31xXSiJgGTCtn1ynAb8u0bZ62/IuK9gFrIuL75ZzrwjKbfpS80tsUAoVp633L2xcR3yons5mZZbm9rpCWR9J8YCNwTX1nMTOzvcs+UUgjolcGxz7Enr2vamZm+4B9opDujZo2asiy3XzkmZmZ1by97q5dMzOzuuRCamZmlgEXUjMzswy4kJqZmWXAhdTMzCwDLqRmZmYZcCE1MzPLgL54+InlEknrgWX1naOaDiX1Rfy5Ipfy5lJWyK28uZQVcitvfWY9KiJa1WSH/kKG3LUsIk6o7xDVIWlermSF3MqbS1kht/LmUlbIrby5lLU6PLVrZmaWARdSMzOzDLiQ5q4J9R1gN+RSVsitvLmUFXIrby5lhdzKm0tZq+SbjczMzDLgEamZmVkGXEhzjKQBkpZJ+oekMfWY4wFJayQtSdvWUtIMScuTnwen7ftJknmZpDPTtveStDjZd6ck1ULWIyX9VdLfJS2VdFW25pW0v6S5khYlWW/M1qxlcjeUtFDSn7I9r6Ti5DxFkuZlc15JLSQ9KemN5N9v7yzOemzyOy19rZN0dbbmrVER4VeOvICGwFvAMUBjYBHQpZ6y9AF6AkvStt0GjEmWxwC3JstdkqxNgKOTa2iY7JsL9AYEvACcVQtZ2wA9k+UDgTeTTFmXN+m3ebLcCPgb8NVszFom9/8DHgP+lM3/FpLzFAOHltmWlXmBh4FLk+XGQItszVomd0Pg/4CjciFvxtdb3wH82o3/WKl/WNPS1n8C/KQe83Rg50K6DGiTLLch9VnXXXIC05JraQO8kbZ9OHBfHeSeDPTP9rzAAcAC4KRszgq0A2YCp/FFIc3mvMXsWkizLi9wEPAOyb0s2Zy1nOxnAHNyJW+mL0/t5pYjgBVp6yuTbdnisIh4HyD52TrZXlHuI5LlsttrjaQOwPGkRnpZmTeZJi0C1gAzIiJrsyb+C7gO2J62LZvzBjBd0nxJl2Vx3mOAD4AHk2nz30lqlqVZy7oAmJgs50LejLiQ5pby3ifIhduuK8pdp9cjqTnwFHB1RKyrrGk52+osb0SUREQBqZHeiZLyK2ler1klnQ2siYj51T2knG11/W/h5IjoCZwFXCmpTyVt6zPvfqTePrknIo4HNpKaGq1INvxukdQYGAxMqqppOdvqPG9NcCHNLSuBI9PW2wGr6ilLeVZLagOQ/FyTbK8o98pkuez2GiepEaki+mhEPJ3teQEi4hOgEBiQxVlPBgZLKgYeB06T9EgW5yUiViU/1wDPACdmad6VwMpkRgLgSVKFNRuzpjsLWBARq5P1bM+bMRfS3PIq8GVJRyd/9V0ATKnnTOmmACOT5ZGk3oss3X6BpCaSjga+DMxNpnnWS/pqclfexWnH1Jik7/uBv0fEr7I5r6RWkloky02BfsAb2ZgVICJ+EhHtIqIDqX+PsyLi29maV1IzSQeWLpN6L29JNuaNiP8DVkg6Ntl0OvB6NmYtYzhfTOuW5srmvJmr7zdp/dq9F/AvpO46fQsYW485JgLvA1tJ/QX5HeAQUjedLE9+tkxrPzbJvIy0O/CAE0j9H9lbwF2UubGihrKeQmpq6DWgKHn9SzbmBboDC5OsS4CfJduzLms52fvyxc1GWZmX1PuOi5LX0tL/DWVx3gJgXvLv4Vng4GzNmpznAOBD4Etp27I2b029/M1GZmZmGfDUrpmZWQZcSM3MzDLgQmpmZpYBF1IzM7MMuJCamZllwIXUzMwsAy6kZmZmGXAhNTMzy8D/B+o4CeoLS8t5AAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|                                                                                          | 0/992 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting examples for words via  yourdictionary\n",
            "---|Processing theme: sdoh\n",
            "------|Lemmas\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|â–Š                                                                                | 10/992 [00:15<25:59,  1.59s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-72-2c34f39defdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# extracting additional examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'yourdictionary'\u001b[0m \u001b[1;31m#@param ['wordnik', 'yourdictionary']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m   \u001b[0mndf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_examples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscheme\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m   \u001b[0mnfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrootdir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdatadir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[0mnfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnfile\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mscheme\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'enh_extract.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-64-eb5cecb12d8d>\u001b[0m in \u001b[0;36mextract_examples\u001b[1;34m(scheme, method, df)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'------|Lemmas'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mndf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'theme'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m \u001b[0mtheme\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mndf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mndf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_more_examples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheme\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mndf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--------|Checkpoint reached. DF saved to ,'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-63-37c31df83548>\u001b[0m in \u001b[0;36mextract_more_examples\u001b[1;34m(theme, method, df, option)\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[1;31m#symptoms.progress_apply(lambda row: get_example(method, row, wordlist), axis=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m   \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaster_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m   \u001b[0mwordlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mget_examples_by_wordrec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m#end of processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    795\u001b[0m                 \u001b[1;31m# on the df using our wrapper (which provides bar updating)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 797\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    798\u001b[0m                 \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[0;32m   7546\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7547\u001b[0m         )\n\u001b[1;32m-> 7548\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7550\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DataFrame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m         \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;31m# wrap results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                     \u001b[1;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                         \u001b[1;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    790\u001b[0m                     \u001b[1;31m# take a fast or slow code path; so stop when t.total==t.n\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                 \u001b[1;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-63-37c31df83548>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[1;31m#symptoms.progress_apply(lambda row: get_example(method, row, wordlist), axis=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m   \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaster_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m   \u001b[0mwordlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mget_examples_by_wordrec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m#end of processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-55-f208e4547224>\u001b[0m in \u001b[0;36mget_examples_by_wordrec\u001b[1;34m(method, df, filename, rec, reclist, save)\u001b[0m\n\u001b[0;32m     39\u001b[0m       }) \n\u001b[0;32m     40\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreclist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m       \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrite_wordlist_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheme\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreclist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#write to file at every 100th record\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m       \u001b[0mreclist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mreclist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-71-d578465f9d59>\u001b[0m in \u001b[0;36mwrite_wordlist_to_file\u001b[1;34m(theme, filename, reclist)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mnew_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpandas\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mappending\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrecords\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m   \"\"\"\n\u001b[1;32m---> 12\u001b[1;33m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwa\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mmy_theme\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheme\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'wa' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3mGGGa1h3-z"
      },
      "source": [
        "# Experiments and manual extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2ZCkWsM4ZjJ",
        "outputId": "94936125-f588-418c-d6ac-5bccc1488213"
      },
      "source": [
        "get_examples_from_yourdictionary('run')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I didn't run away.\",\n",
              " 'I could have run away from my father, as I wanted to.',\n",
              " \"The child can't just run off across the country.\",\n",
              " '\"You\\'re qualified to run the ranch,\" he said.',\n",
              " 'Little calf does run and leap in field.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VTW9we1HvhBY",
        "outputId": "9c6f6e21-bca0-4915-f1ce-6e0528c07846"
      },
      "source": [
        "get_examples_from_wordnik('perspiration')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-74-755ced478208>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_examples_from_wordnik\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'perspiration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-58-c72ec86eb47e>\u001b[0m in \u001b[0;36mget_examples_from_wordnik\u001b[1;34m(word, elimit)\u001b[0m\n\u001b[0;32m     11\u001b[0m   \u001b[0mwordApi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordApi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWordApi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mexamplesSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordApi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetExamples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0melimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincludeDuplicates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reason'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\site-packages\\wordnik\\WordApi.py\u001b[0m in \u001b[0;36mgetExamples\u001b[1;34m(self, word, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mpostData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m'body'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         response = self.apiClient.callAPI(resourcePath, method, queryParams,\n\u001b[0m\u001b[0;32m     76\u001b[0m                                           postData, headerParams)\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\site-packages\\wordnik\\swagger.py\u001b[0m in \u001b[0;36mcallAPI\u001b[1;34m(self, resourcePath, method, queryParams, postData, headerParams)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Make the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mrequest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequestParams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_content_charset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'urllib.Request'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;31m# post-process response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[0;32m    543\u001b[0m                                   '_open', req)\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPConnection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m     \u001b[0mhttp_request\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0m\u001b[0;32m   1351\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0;32m   1352\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1253\u001b[0m                 encode_chunked=False):\n\u001b[0;32m   1254\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1299\u001b[0m             \u001b[1;31m# default charset of iso-8859-1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1301\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1248\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m     def request(self, method, url, body=None, headers={}, *,\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1008\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb\"\\r\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotConnected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    919\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m         \u001b[1;34m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m         self.sock = self._create_connection(\n\u001b[0m\u001b[0;32m    922\u001b[0m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0;32m    923\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\tf_keras_3\\lib\\socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    794\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m                 \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 796\u001b[1;33m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    797\u001b[0m             \u001b[1;31m# Break explicitly a reference cycle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m             \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPEv064UofeA"
      },
      "source": [
        "Add more examples to symtoms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zllMMNpIl-D4"
      },
      "source": [
        "#themes = tdf.theme.nunique( )\n",
        "theme = 'symptoms' # ['risk-factors', 'related-terms','ssdoh', 'diet', 'symptoms', 'prevalence', 'physical-activity']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpT4snYXs5zx"
      },
      "source": [
        "theme = 'symptoms'\n",
        "wordlist = extract_more_examples(theme, method)\n",
        "columns=['theme', 'parent_word', 'confidence', 'synset_name', 'lex', 'pos', 'text']\n",
        "ndf = pd.DataFrame(columns=columns)\n",
        "ndf = ndf.append(wordlist)\n",
        "examples_file =  rootdir + os.sep + 'data' + os.sep + method +'_'+ theme + scheme + '.csv'\n",
        "ndf.to_csv(examples_file)\n",
        "tdf = tdf.append(wordlist)\n",
        "draw_plot(tdf)\n",
        "full_file = rootdir + os.sep + 'data' + os.sep + 'wn_themes' + scheme + '_2.csv'\n",
        "tdf.to_csv(full_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1xQNQwVo6DP"
      },
      "source": [
        "wordlist = extract_more_examples_and_save('symptoms', method, option=2)\n",
        "tdf = tdf.append(wordlist)\n",
        "draw_plot(tdf)\n",
        "full_file = rootdir + os.sep + 'data' + os.sep + 'wn_themes' + scheme + '_2.csv'\n",
        "tdf.to_csv(full_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaIxdqpnwhpI"
      },
      "source": [
        "draw_plot(tdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgGQw1wCooSv"
      },
      "source": [
        "Add more examples to risk-factors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQn99bTGrM3x"
      },
      "source": [
        "theme = 'risk-factors'\n",
        "wordlist = extract_more_examples(theme, method)\n",
        "columns=['theme', 'parent_word', 'confidence', 'synset_name', 'lex', 'pos', 'text']\n",
        "ndf = pd.DataFrame(columns=columns)\n",
        "ndf = ndf.append(wordlist)\n",
        "examples_file =  rootdir + os.sep + 'data' + os.sep + method +'_'+ theme + scheme + '.csv'\n",
        "ndf.to_csv(examples_file)\n",
        "tdf = tdf.append(wordlist)\n",
        "draw_plot(tdf)\n",
        "full_file = rootdir + os.sep + 'data' + os.sep + 'wn_themes' + scheme + '_2.csv'\n",
        "tdf.to_csv(full_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdpMY6RsyIvo"
      },
      "source": [
        "theme = 'risk-factors'\n",
        "wordlist = extract_more_examples_and_save(theme, method, option=2)\n",
        "tdf = tdf.append(wordlist)\n",
        "draw_plot(tdf)\n",
        "full_file = rootdir + os.sep + 'data' + os.sep + 'wn_themes' + scheme + '_2.csv'\n",
        "tdf.to_csv(full_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VTH9kW9ovEv"
      },
      "source": [
        "Add more examples to related-terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pwdlq567G6Y"
      },
      "source": [
        "theme = 'related-terms'\n",
        "wordlist = extract_more_examples(theme, method)\n",
        "columns=['theme', 'parent_word', 'confidence', 'synset_name', 'lex', 'pos', 'text']\n",
        "ndf = pd.DataFrame(columns=columns)\n",
        "ndf = ndf.append(wordlist)\n",
        "examples_file =  rootdir + os.sep + 'data' + os.sep + method +'_'+ theme + scheme + '.csv'\n",
        "ndf.to_csv(examples_file)\n",
        "tdf = tdf.append(wordlist)\n",
        "draw_plot(tdf)\n",
        "full_file = rootdir + os.sep + 'data' + os.sep + 'wn_themes' + scheme + '_2.csv'\n",
        "tdf.to_csv(full_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh3btTODzHSg"
      },
      "source": [
        "theme = 'related-terms'\n",
        "wordlist = extract_more_examples_and_save(theme, method, option=2)\n",
        "tdf = tdf.append(wordlist)\n",
        "draw_plot(tdf)\n",
        "full_file = rootdir + os.sep + 'data' + os.sep + 'wn_themes' + scheme + '_2.csv'\n",
        "tdf.to_csv(full_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJaZ5-sjox8_"
      },
      "source": [
        "Add more examples to prevalence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w82a5BLmMHVv"
      },
      "source": [
        "theme = 'prevalence'\n",
        "wordlist = extract_more_examples(theme, method)\n",
        "columns=['theme', 'parent_word', 'confidence', 'synset_name', 'lex', 'pos', 'text']\n",
        "ndf = pd.DataFrame(columns=columns)\n",
        "ndf = ndf.append(wordlist)\n",
        "examples_file =  rootdir + os.sep + 'data' + os.sep + method +'_'+ theme + scheme + '.csv'\n",
        "ndf.to_csv(examples_file)\n",
        "tdf = tdf.append(wordlist)\n",
        "draw_plot(tdf)\n",
        "full_file = rootdir + os.sep + 'data' + os.sep + 'wn_themes' + scheme + '_2.csv'\n",
        "tdf.to_csv(full_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HD-rpqlyYZf"
      },
      "source": [
        "theme = 'prevalence'\n",
        "wordlist = extract_more_examples_and_save(theme, method, option=2)\n",
        "tdf = tdf.append(wordlist)\n",
        "draw_plot(tdf)\n",
        "full_file = rootdir + os.sep + 'data' + os.sep + 'wn_themes' + scheme + '_2.csv'\n",
        "tdf.to_csv(full_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaDzpY9lzNhQ"
      },
      "source": [
        "Add more example to physical-activity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj3myPDOzQuX"
      },
      "source": [
        "full_file = rootdir + os.sep + 'data' + os.sep + 'wn_themes' + scheme + '_2.csv'\n",
        "theme = 'physical-activity'\n",
        "wordlist = extract_more_examples_and_save(theme, method, option=1)\n",
        "tdf = tdf.append(wordlist)\n",
        "tdf.to_csv(full_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYirKqkIO0kp"
      },
      "source": [
        "wordlist = extract_more_examples_and_save(theme, method, option=2)\n",
        "tdf = tdf.append(wordlist)\n",
        "tdf.to_csv(full_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3OsV8FNO3EQ"
      },
      "source": [
        "draw_plot(tdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YZ3xo7Zzy4Y"
      },
      "source": [
        "Add more examples to sdoh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol6a16xVz2Vf"
      },
      "source": [
        "full_file = rootdir + os.sep + 'data' + os.sep + 'wn_themes' + scheme + '_2.csv'\n",
        "theme = 'sdoh'\n",
        "wordlist = extract_more_examples_and_save(theme, method, option=1)\n",
        "tdf = tdf.append(wordlist)\n",
        "tdf.to_csv(full_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G86xQdxKWJPI"
      },
      "source": [
        "wordlist = extract_more_examples_and_save(theme, method, option=2)\n",
        "tdf = tdf.append(wordlist)\n",
        "tdf.to_csv(full_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul84JcxjWLJv"
      },
      "source": [
        "draw_plot(tdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Nyf7Zw0z74_"
      },
      "source": [
        "Add more examples to diet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "764qVNyYz-SH"
      },
      "source": [
        "scheme = 'scheme2' #@param ['scheme1', 'scheme2']\n",
        "full_file = rootdir + os.sep + 'data' + os.sep + 'wn_themes' + scheme + '_2.csv'\n",
        "tdf = read_master_file(full_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GljILvI0ka5"
      },
      "source": [
        "draw_plot(tdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZHC8W662DiU"
      },
      "source": [
        "theme = 'diet'\n",
        "len(tdf[(tdf['theme']== theme) & (tdf['ttype']=='related')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Inr8yDtl0rE0"
      },
      "source": [
        "extract_more_examples_and_save('diet', method, tdf, full_file, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xaCLR4I28hV"
      },
      "source": [
        "extract_more_examples_and_save('diet', method,tdf, full_file, option=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q4lFt_UtGlx"
      },
      "source": [
        "draw_plot(tdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPqUPMtXnEgA"
      },
      "source": [
        "print('number of lines from other sources = ', len(tdf[tdf['source']!='wordnet']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pELyvVPU2eY"
      },
      "source": [
        "print('Number of texts which are missing are :', len(tdf[tdf.text =='']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZGjoLDlVBa1"
      },
      "source": [
        "tdf[tdf.theme=='symptoms']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQaUAhnyVwq0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "6ec38905-21ac-4832-edbf-364a06cc2cf5"
      },
      "source": [
        "tdf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               theme parent_word  confidence      synset_name             lex  \\\n",
              "0               sdoh        life         0.7        life.n.02  noun.cognition   \n",
              "1               sdoh        life         0.7        life.n.02  noun.cognition   \n",
              "3               sdoh        life         0.7        life.n.02  noun.cognition   \n",
              "4               sdoh      living         1.0      living.n.02      noun.group   \n",
              "6               sdoh      living         1.0      living.n.02      noun.group   \n",
              "...              ...         ...         ...              ...             ...   \n",
              "31891  related-terms        soup         0.5        soup.v.01       verb.body   \n",
              "31892  related-terms        soup         0.5        soup.v.01       verb.body   \n",
              "31894  related-terms  intoxicate         0.5  intoxicate.v.03       verb.body   \n",
              "31895  related-terms  intoxicate         0.5  intoxicate.v.03       verb.body   \n",
              "31896  related-terms  intoxicate         0.5  intoxicate.v.03       verb.body   \n",
              "\n",
              "      pos                                               text   source  \\\n",
              "0       n  the experience of being alive; the course of h...  wordnet   \n",
              "1       n                                        life,living  wordnet   \n",
              "3       n  he could no longer cope with the complexities ...  wordnet   \n",
              "4       n                        people who are still living  wordnet   \n",
              "6       n                      save your pity for the living  wordnet   \n",
              "...    ..                                                ...      ...   \n",
              "31891   v                                 dope (a racehorse)  wordnet   \n",
              "31892   v                                               soup  wordnet   \n",
              "31894   v          have an intoxicating effect on, of a drug  wordnet   \n",
              "31895   v                                         intoxicate  wordnet   \n",
              "31896   v                            intoxicant,intoxication  wordnet   \n",
              "\n",
              "            ttype  \n",
              "0      definition  \n",
              "1          lemmas  \n",
              "3         example  \n",
              "4      definition  \n",
              "6         example  \n",
              "...           ...  \n",
              "31891  definition  \n",
              "31892      lemmas  \n",
              "31894  definition  \n",
              "31895      lemmas  \n",
              "31896     related  \n",
              "\n",
              "[16061 rows x 9 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>theme</th>\n",
              "      <th>parent_word</th>\n",
              "      <th>confidence</th>\n",
              "      <th>synset_name</th>\n",
              "      <th>lex</th>\n",
              "      <th>pos</th>\n",
              "      <th>text</th>\n",
              "      <th>source</th>\n",
              "      <th>ttype</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sdoh</td>\n",
              "      <td>life</td>\n",
              "      <td>0.7</td>\n",
              "      <td>life.n.02</td>\n",
              "      <td>noun.cognition</td>\n",
              "      <td>n</td>\n",
              "      <td>the experience of being alive; the course of h...</td>\n",
              "      <td>wordnet</td>\n",
              "      <td>definition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sdoh</td>\n",
              "      <td>life</td>\n",
              "      <td>0.7</td>\n",
              "      <td>life.n.02</td>\n",
              "      <td>noun.cognition</td>\n",
              "      <td>n</td>\n",
              "      <td>life,living</td>\n",
              "      <td>wordnet</td>\n",
              "      <td>lemmas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sdoh</td>\n",
              "      <td>life</td>\n",
              "      <td>0.7</td>\n",
              "      <td>life.n.02</td>\n",
              "      <td>noun.cognition</td>\n",
              "      <td>n</td>\n",
              "      <td>he could no longer cope with the complexities ...</td>\n",
              "      <td>wordnet</td>\n",
              "      <td>example</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sdoh</td>\n",
              "      <td>living</td>\n",
              "      <td>1.0</td>\n",
              "      <td>living.n.02</td>\n",
              "      <td>noun.group</td>\n",
              "      <td>n</td>\n",
              "      <td>people who are still living</td>\n",
              "      <td>wordnet</td>\n",
              "      <td>definition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>sdoh</td>\n",
              "      <td>living</td>\n",
              "      <td>1.0</td>\n",
              "      <td>living.n.02</td>\n",
              "      <td>noun.group</td>\n",
              "      <td>n</td>\n",
              "      <td>save your pity for the living</td>\n",
              "      <td>wordnet</td>\n",
              "      <td>example</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31891</th>\n",
              "      <td>related-terms</td>\n",
              "      <td>soup</td>\n",
              "      <td>0.5</td>\n",
              "      <td>soup.v.01</td>\n",
              "      <td>verb.body</td>\n",
              "      <td>v</td>\n",
              "      <td>dope (a racehorse)</td>\n",
              "      <td>wordnet</td>\n",
              "      <td>definition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31892</th>\n",
              "      <td>related-terms</td>\n",
              "      <td>soup</td>\n",
              "      <td>0.5</td>\n",
              "      <td>soup.v.01</td>\n",
              "      <td>verb.body</td>\n",
              "      <td>v</td>\n",
              "      <td>soup</td>\n",
              "      <td>wordnet</td>\n",
              "      <td>lemmas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31894</th>\n",
              "      <td>related-terms</td>\n",
              "      <td>intoxicate</td>\n",
              "      <td>0.5</td>\n",
              "      <td>intoxicate.v.03</td>\n",
              "      <td>verb.body</td>\n",
              "      <td>v</td>\n",
              "      <td>have an intoxicating effect on, of a drug</td>\n",
              "      <td>wordnet</td>\n",
              "      <td>definition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31895</th>\n",
              "      <td>related-terms</td>\n",
              "      <td>intoxicate</td>\n",
              "      <td>0.5</td>\n",
              "      <td>intoxicate.v.03</td>\n",
              "      <td>verb.body</td>\n",
              "      <td>v</td>\n",
              "      <td>intoxicate</td>\n",
              "      <td>wordnet</td>\n",
              "      <td>lemmas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31896</th>\n",
              "      <td>related-terms</td>\n",
              "      <td>intoxicate</td>\n",
              "      <td>0.5</td>\n",
              "      <td>intoxicate.v.03</td>\n",
              "      <td>verb.body</td>\n",
              "      <td>v</td>\n",
              "      <td>intoxicant,intoxication</td>\n",
              "      <td>wordnet</td>\n",
              "      <td>related</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16061 rows Ã— 9 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjM4QDeA1GDz"
      },
      "source": [
        "len(tdf[tdf['theme']=='sdoh' & tdf['ttype']=='lemmas'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx2cEWdI1OwZ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}